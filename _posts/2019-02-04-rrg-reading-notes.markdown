---
layout: post
title: "February 04 2019 RRG Notes"
date: 2019-02-04 09:00 -0600
categories: rrg_notes
---

## [Four things you already agree with (that mean you're probably on board with effective altruism)](https://www.givingwhatwecan.org/post/2016/03/four-things-you-already-agree-with-effective-altruism/)

+ Four ideas you probably already agree with
    + It's important to help others -- when people are in need and we're able to help, we should help
    + People are equal -- everyone has an equal claim to being happy, healthy, fulfilled and free
    + Helping more is better than helping less -- we should save more lives, help people live longer and help people live happier lives
        + If you had enough medicine to save 20 people, and no reason to conserve, would you choose to only help 10 people
    + Our resources are limited
        + We can only choose to spend so much time and so much money
        + Choosing to spend time or money on one problem is implicitly choosing to no spend it on another problem
+ These four ideas are pretty uncontroversial
+ Defending the opposite positions would be difficult and uncomfortable
    + Helping others isn't morally required or even that good
    + It's okay to value people differently based on arbitrary differences
    + It doesn't matter if some people die even if it doesn't cost us anything to save their lives
    + We have unlimited resources
+ If these four ideas embody important values then we're probably thinking about doing good totally wrong
+ We need to think about how we can help the most people with the resources that we have
+ The difference in impact between can be dramatic -- the most impactful charities can have an impact that tens or hundreds of times as large as the least impactful
+ A charity chose at random is probably not making as much of an impact as the most impactful charities
+ The charities that we donate to are, in effect, chosen at random -- we choose charities based upon what we see, read, and hear about from our friends
+ Every worthy cause should be on the table
    + Climate justice
    + Animal sanctuaries
    + Preventing easily treatable but unpronounceable diseases in place we've never heard of and will probably never visit
+ Trying to be neutral is really difficult -- it's hard to not favor causes that have had a personal impact on yourself or your family
+ However, if we care about treating people equally, we should care about treating their experiences equally
+ We need to treat _all_ suffering and death as tragedy, not just the death and suffering we happen to see
+ EA is a way to better fulfill the values that you already hold
+ EA asks us to face up to some hard choices, but we're making those choices whether we think about them or not
+ Even though it might feel difficult to not donate to a charity that seems worthy, remember that you're always trading off causes against one another

## [What Is Utilitarianism](http://dragice.fr/utilitarianism/faq.html#WHATIS)

+ Utilitarianism is a collection of philosophical positions which have the following 5 characteristics in common
    1. Universalism
        + Moral principles are universal
        + Same moral standards apply to all people and all situations
        + The utility of all people is important, and is, in fact, equally important
    2. Consequentialism
        + What matters, morally speaking is the consequences of actions
        + Actions aren't inherently good or inherently bad
        + Fairly controversial belief -- many people believe that there are actions that are wrong, regardless of their consequences
    3. Welfarism
        + Good consequences are that which improve the welfare of people
        + Well-being is defined subjectively, and the definition differs between utilitarian philosophies
        + A belief opposed to welfarism would hold that there are principles which are important, even if they don't benefit anyone in the particular situation being discussed
    4. Aggregation
        + Utilitarianism is an aggregative philosophy
        + What is good overall is the aggregation of what is good for each individual
        + This is a fairly controversial principle, as it implies that the welfare of people can always be compared
    5. Maximization
        + Utilitarianism is a maximalist philosophy
        + What ever utility is defined as, it's better to have more of it than less
+ Given these characterisitics, utilitarianism holds that
    + Morality of actions is solely judged by how those actions maximize utility
    + Utility is the welfare of individual people from the perspective of those people
    + One person's welfare is as important as any other person's
+ Non-utilitarian philosophies hold that:
    + Actions can be right or wrong regardless of their consequences
    + Some consequences are good even if they don't increase the welfare of any individual
    + We should promote welfare in some way other than maximization

## [The Drowning Child and the Expanding Network](https://www.utilitarian.net/singer/by/199704--.htm)

+ Imagine you're walking past a shallow pond and you see that a child has fallen in
+ Do you have an obligation to rescue the child even though it would result in yoru clothes getting ruined and you being late to school/work?
+ Most people would answer that you do have an obligation to rescue the child
+ Does it make a difference if the child is far away, in another country?
+ At this point, most people challenge the practicalities
    + Can we be sure that we're actually helping?
    + Isn't the real problem something else, like growing world population?
+ Hardly anyone challenges the underlying ethics
+ For most of human history, there was simply no way to affect the lives of those living hundreds or thousands of miles away
+ The 20th century is the first century in which it's been possible to think about a truly global community and global responsibilities
+ Not only can we have impacts on others far away, but via environmental degradation, we _are_ having effects
+ The 20th century is also the century in which the big ideas that gave meaning to our lives died and were replaced by capitalism
+ The opposition of ethics with self-interest is a consequence of this
+ Capitalism's only message is to consume ever more in a never-ending race to have more than your neighbor
+ Identifying with other, larger goals can lend meaning to our lives and can reconcile ethics with self-interest
+ If we can identify our self-interest with the larger interests of humanity and the natural world as a whole, then we are freed from the need to consume ever more in order to stay ahead of our peers
+ _Editor's Note: LessWrong had a fairly [spirited discussion](https://www.greaterwrong.com/posts/M8zgMmNCfpQxaKo8Y/some-reservations-about-singer-s-child-in-the-pond-argument) regarding Peter Singer's pond argument in 2013. The thread is well worth a read._

## [If you want to disagree with effective altruism, you need to disagree with one of these three claims](https://ea.greaterwrong.com/posts/toAMJ3cWQiWDheaKD/if-you-want-to-disagree-with-effective-altruism-you-need-to)

+ Effective altruism is often motivated by referring the Peter Singer's pond argument
+ This is a mistake
    + Associates EA with with international development
    + Makes it appear that if you can refute the pond argument, you can refute EA
+ EA is justified not by the pond argument but by a weaker general form of the pond argument
    + The original form of the pond argument is:
        + If you can help others a great deal without sacrificing something of similar significance, you should
        + We can help the global poor a great deal by giving to effective charities
        + Therefore, we ought to give to effective charities until it becomes a great sacrifice
    + This leads to the objection of wondering whether international aid really helps the global poor
    + However, one can deny the effectiveness of international aid and still accept the importance of effective altruism
    + As long as there are some actiosn which benefit others a great deal, but which cost us little ("pond-like actions") EA will be important
    + This leads to the "general pond argument"
        + As long as there are actions which benefit others a great deal, but which cost us little, we should do them
        + Some of these actions are not widely taken
        + Therefore, there are cost-effective and highly-beneficial (pond-like) actions that we could be taking, but are not
    + The mission of effective altruism is to find these pond-like actions and funnel resources towards them
+ Why do we think there will be lots of these actions?
    + Global inequality
        + College graduates in developed countries are roughly 100 times as rich as the global poor
        + That means that these people could do 100 times as much good by transferring money to the global poor than by spending that money on themselves
        + Moreover, there are probably ways of helping that are more efficient than a straight income transfer, so in reality the ratio is probably greater than 100x
    + Moral concern for animals
        + Animals historically have had no political or economic power
        + As a result, people have not cared about animals' interests in the past
        + By doing something simple like going vegetarian, you personally prevent the deaths of something like 100 animals a year
        + There are probably other actions that are equally simple which are as impactful, if not more
    + The ability to affect the future
        + There will be many more people living in the future than are living today
        + If you believe that we should have moral concern about the welfare of future generations, and believe that our actions today can affect them, then there are relatively small actions that you can take today which will have disproportionate consequences on future generations
    + The possibility of leverage
        + If you focus on the finding the best ways to help others you can often find ways of doing good which are more effective than just doing good things by yourself
        + If you think some action A is good, then you can probably convince 10 other people to also do A
        + Then you've just magnified your own impact by 10x
    + Poor existing methods
        + Many current attempts to do good aren't strategic or evidence-based
        + There are probably ways to do good which are 10 or even 100 times as impactful as the methods that people normally focus on
+ How _not_ to refute the importance of effective altruism
    + To disagree with effective altruism, you need to disagree with one of the parts of the "general pond argument"
    + Most critiques of EA fail to do this
    + Common failure modes
        + Equating effective altruism with utilitarianism
            + EA rests on a much weaker moral claim than utilitarianism
            + EA merely says that you ought to do actions that are a great benefit to others with little cost to yourself
            + In contrast, utilitarianism says that you ought to do an action that's a major sacrifice, as long as it it does slightly more good to others than it does harm to yourself
            + Utilitarianism also denies that anything matters except welfare, and that it's okay to violate rights in favor of the greater good
        + Arguing that a specific action is not pond-like
            + This is not a general critique of EA
            + In fact, this contributes to EA's ability to find the best missions to support
        + Saying that EA says that you should only support charities that have randomized controlled trial evidence backing them
            + RCTs are just a tool
            + There are other ways of identifying effective charities
    + What kinds of critiques might hit the mark?
        + Denying the moral claim
            + However this implies that you would let the child drown in the pond rather than ruin some of your possessions
            + This is an uncomfortable claim for most people to make
        + Show that there's an important moral difference between saving the child in the pond in front of you and _all_ the other pond-like actions you could be taking
            + In other words, you have to show that there's something special about the situation with the child drowning in front of you that does not apply to all the other situations in which people's lives can be saved for relatively little cost
            + This is more difficult than showing there's a difference between any specific action (like donating to charities) and saving the drowning child
        + Accept that effective altruism is correct as a principle but deny that the effective altruism movement will do much good
+ Conclusion
    + EA needs to discuss a wider range of issue than just donating to international health charities
    + The core idea that EA needs to communicate is that there exist cost effective ways to save lives
    + If we can do this, we can make the case for EA in a much more robust fashion than by focusing on specific actions or charities

## [On Caring](http://mindingourway.com/on-caring/)

+ It's difficult to feel the size of large numbers
+ A billion feels just a bit larger than a million, even though in reality it's a thousand times larger
+ This insensitivity matters because sometimes the things you care about are really numerous
+ The human brain is simply incapable of generating enough caring to encompass the suffering in the world
+ Caring about the world isn't about having a gut-feeling that corresponds to the amount of actual suffering in the world; it's about not having that feeling and then working to address the suffering anyway
+ Our built-in heuristics completely fail to grasp the stakes we're playing for
    + Billions of people suffering today
    + The lives of the trillions or quadrillions that will exist in the future
+ Saving one life feels as good as saving the entire world
+ There's a mental shift that occurs when you internalize this scope-insensitivity
    + Notice that most charitable donations occur in a social context
    + We agree that people should give something to charity, but when we see someone give everything, we become uncomfortable
+ When some people internalize this scope insensitivity, they freeze up
    + See that there's nothing they can do to affect the world's biggest problems
    + Freeze up, since the problems are so large, so numerous, and there's so little that one can do
+ Most of us go through life understanding that we should care about people far away, but actually failing to care
+ This is an error -- instead of thinking we need to care first before donating, we should donate to good causes whether or not we feel like we care about them
+ So if you can't use how much you care as a heuristic, nor can you use social pressure, how should you be deciding what causes to donate to?
    + Not sure yet
    + GiveWell, MIRI, FHI, etc. are all attempts to answer this question
+ Nobody can care enough to truly comprehend the problems we face
+ So instead of using care as a heuristic, [shut up and multiply](https://www.greaterwrong.com/posts/4ZzefKQwAtMo5yp99/circular-altruism), and then donate to what the math tells you to donate to, even though you might not personally care
