---
layout: post
title: "December 19 2016 RRG Notes"
date: 2016-12-18 20:00 -0800
categories: rrg_notes
---

## Reading Notes

### [Nobody is Perfect - Everything Is Commensurable](http://slatestarcodex.com/2014/12/19/nobody-is-perfect-everything-is-commensurable/)
* The [toxoplasma of rage](http://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/) does have one benefit - it makes people uncomfortable, which is one prerequisite for getting them to change
* Activist vs. passivist politics
  * Scott takes a passivist approach to politics 
  * Repulsed by protests - doesn't like the thought of people gathering specifically to demonize and vilify an other
  * Others are too sensitive for activist politics - the notion that failing to take action to prevent a bad thing makes that bad thing your fault is psychologically triggering for many
* So do we deny the obligation to help?
  * After all few of us impose a non-negligible marginal cost on the poor
  * However, there does seem to be a deep-seated human desire to express gratitude and help the less fortunate
* Why do we privilege political action when we consider ways of improving the world?
  * Compare attending a protest to donating to charity - charity is two or three orders of magnitude more efficient at improving peoples' lives
* How much should we give?
  * 10% is a reasonable Schelling point
  * Donations don't have to be entirely in terms of money - volunteer time works as well
* If everybody gave 10% of their income to charities, [Moloch](http://slatestarcodex.com/2014/07/30/meditations-on-moloch/) would die a swift death
  * Moloch is the spirit of people responding to perverse incentives
  * Giving to charity is an unincentivized action - anathema to Moloch
* Giving what you can individually to charity allows you to do good even when you find political action and mass movements to be uncomfortable or disturbing

### [How We Talk About What Donations Will Achieve](https://www.givingwhatwecan.org/post/2016/07/how-we-talk-about-what-donations-will-achieve/)
* Separating charities and interventions allows charities to plug gaps in large scale interventions
* Example:
  * AMF only purchases bed nets and does some regional distribution
  * Relies on local partners to do individual distribution and need-assessment
  * SCI doesn't actually perform deworming treatment - only provides technical support and purchases medicine
* Separating charities from overall interventions allows charities to specialize and become more efficient
* We should evaluate charities not by their individual efficiency, but by the efficiency of the overall intervention that they are a part of

### [The Long-Term Significance of Reducing Global Catastrophic Risk](http://blog.givewell.org/2015/08/13/the-long-term-significance-of-reducing-global-catastrophic-risks/)
* Global catastrophic risks can permanently negatively affect the trajectory of civilization, even when they don't lead to total extinction of humanity
* Why should we focus on catastrophic risks that may cause large numbers of deaths without causing humanity to die out?
  * The probability of extinction-level events is small, compared to the probability of events that may cause hundreds of millions of deaths
  * If a large number of people are wiped out, there is a possibility that civilization will not recover, or will recover in a way that stunts humanity's long-term potential
  * The combination of the above two factors means that threats that wipe out a significant fraction of humanity without causing extinction pose some of the same risks as extinction-level threats
* Basic framework and terms:
  * 2 possible frameworks:
    * Minimize expected deaths
    * Maximize long term potential of humanity
  * This article will be using the latter framework
  * 2 levels of catastrophic risk
    * Level 1: deaths of hundreds of millions or some billions
    * Level 2: Complete extinction of humanity
  * 2 schools of thought:
    * Level-2 focus: efforts at minimizing global catastrophic risk should focus on level 2 events exclusively
    * Dual focus: level 1 and level 2 events pose similar levels of threat to the long term development of humanity
  * Why dual focus: there is some non-zero probability that civilization will not recover from a level 1 event
* Global catastrophes are more likely than extinctions
  * Possible catastrophes:
    * Pandemic
    * Nuclear war
    * Climate change
    * Geoengineering
    * AI
  * In all of these scenarios (except maybe AI) a level 1 catastrophe seems more likely than full extinction
* General reasons to think that a global disruption might affect the distant future
  * The world has had unusually positive civilizational progress over the last few hundred years
  * There is little consensus about the mechanisms underlying civilizational progress
    * Is the Industrial Revolution inevitable?
  * There is essentially no precedent for a level 1 catastrophe
    * The closest thing is the Black Death in Europe, and that occurred before the Industrial Revolution
* Specific mechanisms by which a catastrophe could affect the distant future
  * Disruption of sustained scientific progress
    * Scientific progress requires
      * Scientists
      * People willing to learn science
      * People willing to turn scientific discoveries into inventions
      * Institutional support for the above
    * A disruption of any of the factors necessary for scientific progress could lead to the following failures:
      * Risky stall:
        * A stall in scientific progress makes humanity vulnerable to an extinction level catastrophic risk
      * Resource depletion and environmental degradation
        * Humanity runs out of some essential resource or degrades its environment too much due to insufficient scientific advancement
      * Permanent stagnation
        * Maybe we lose science forever and are just stuck at some particular level of development
  * Disruption of sustained social progress
    * Negative cultural trajectory - closed authoritarian societies gain some kind of permanent edge over open societies
      * Would probably lead to a reduced rate of scientific progress
    * Increased inter-state violence
    * Irrevocable technological mistakes
      * Weaponized AI
      * Biological weapons
* Potential offsetting factors
  * Could a level 1 catastrophe make humanity more resilient against level 2 catastrophes?
  * Probably not
    * Reactions to level-1 catastrophes are probably over-specific to that catastrophe
    * Overall, industrialization has been a net positive for humanity, even taking into account environmental degradation
    * A level 1 catastrophe seems as likely to set back preparations for dealing with critical technological junctures as it is to allow more time for preparations
* Conclusions and strategic implications
  * Events with hundreds of millions dead are much more likely than extinction events
  * These events could derail civilizational progress
  * There is essentially no precedent for events of this scale
  * Therefore a dual approach to X-Risk is warranted

### [Cosmopolitanism](http://effective-altruism.com/ea/6w/cosmopolitanism/)
* Cosmopolitanism is a moral stance that gives the interests of other nationalities a weight equal to people of one's own nationality
* Pretty much unanimous support for cosmopolitanism among EAs
* However, most other people do not believe in cosmopolitanism
* "Anti-foreign/anti-immigrant" doesn't carry nearly the same level of stigma as "racist"
* Analyzing policy with a cosmopolitan lens can be enlightening
  * Allows EAs to find allies in places that they may not expect
* What does cosmopolitanism say about policy?
  * Countries should give more moral weight to the citizens of other countries, especially when waging war
  * Cosmopolitan civilian test for proportionality in war - would civilian casualties be considered proportionate if the civilians were of a different nationality?
* EA should be more open and explicit about cosmopolitanism