*--
layout: post
title: "May 15 2017 RRG Notes"
date: 2017-05-12 19:00 -0700
categories: rrg_notes
*--

## [What Would You Do Without Morality](http://lesswrong.com/lw/rq/what_would_you_do_without_morality/)
* What if you were told convincingly that nothing is immoral?
* Would your behavior change?
* If so, how would it change?

## [Changing Your Metaethics](http://lesswrong.com/lw/sk/changing_your_metaethics/)
* Ethics: "murder is bad"
* Meta-ethics: "murder is bad because..."
* It's possible for people to agree on points of ethics while being in strong disagreement on points of metaethics
* In order to make philosophical progress, you need to be able to change your metaethics without losing your sense of ethics entirely
* To allow yourself to change metaethics, it's useful to set up lines of retreat that allow you to change your meta-ethics without immediately affecting your sense of ethics
* Lines of possible retreat:
	- [The Moral Void](http://lesswrong.com/lw/rr/the_moral_void/) - if your metaethics tells you that some part of your ethics is wrong, you can allow your ethics to override your sense of meta-ethics
	- Lines of retreat for naturalistic meta-ethics:
		- [Joy in the merely real](http://lesswrong.com/lw/or/joy_in_the_merely_real/), [Explaining vs. Explaining Away](http://lesswrong.com/lw/oo/explaining_vs_explaining_away/) - things don't have to be mysterious to be beautiful
		- [No Universally Compelling Arguments](http://lesswrong.com/lw/rn/no_universally_compelling_arguments/) - your meta-ethics don't have to be universally adopted, or even universally adoptable in order to be valid
		- [Where Recursive Justification Hits Bottom](http://lesswrong.com/lw/s0/where_recursive_justification_hits_bottom/), [My Kind of Reflection](http://lesswrong.com/lw/s2/my_kind_of_reflection/) - difference between circular logic and self-consistent loops through the meta-level
		- [The Gift We Give To Tomorrow](http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/)
			- It's okay for evolution to have created our sense of morality
			- The fact that our moral sense is the result of an amoral process doesn't automatically imply that our built-in moral sense is wrong or bad
		- [Evolutionary Psychology](http://lesswrong.com/lw/l1/evolutionary_psychology/) - evolution does not and should not be the last word morality
		- [Does Morality Care What You Think](http://lesswrong.com/lw/sj/does_your_morality_care_what_you_think/), [Math is Subjunctively Objective](http://lesswrong.com/lw/si/math_is_subjunctively_objective/), [Probability is Subjectively Objective](http://lesswrong.com/lw/s6/probability_is_subjectively_objective/) - the fact that morality is developed by limited and imperfect human minds doesn't automatically imply that the meta-ethical system has to be imperfect
		- [Existential Angst Factory](http://lesswrong.com/lw/sc/existential_angst_factory/) - if you think living in a universe of mere particles is boring, maybe your life is just boring
* Why is meta-ethics important
	- People think that rationality drains the meaning and wonder out of the universe
	- Having a false understanding of where ethics comes from causes distress

## [Could Anything Be Right](http://lesswrong.com/lw/sb/could_anything_be_right/)
* Can you trust any of your moral instincts?
* Your belief that you have no information about morality is not internally consistent
* If you can't come up with a procedure to define morality, how can you program an AI to decide morality?
* If you discard everything evolution has to say about morality, you discard yourself; every part of you (including the parts of your brain that think about morality) is a product of evolution
* You should take your initial notion of morality as a starting position
* Morality is a product of reason; it doesn't come from some light shining from beyond

## [Morality as Fixed Computation](http://lesswrong.com/lw/sw/morality_as_fixed_computation/)
* If you tell an all powerful AI to give you what you want, the AI will modify you to want something cheap, and then give you that
* The problem is that we don't have enough insight into our own psychology and neurology to fully know and describe why we want what we want
* This situation analogizes to moral philosophy if you replace the question of "What do I want?" with "What is right?"
* The notion of "right" is a fixed question/fixed framework
* Morality isn't tautologically defined by what you want
* Instead what you want and what is moral are both linked to some underlying value system
* If the AI changes what you desire, that underlying value system doesn't change, and so what is moral doesn't change
* _Note: I was confused by this article, so I apologize in advance if I misinterpreted Eliezer or oversimplified his argument_

## [The True Prisoners' Dilemma](http://lesswrong.com/lw/tn/the_true_prisoners_dilemma/)
* The normal formulation of a Prisoner's Dilemma involves two humans and a symmetric payoff matrix
* The problem with this formulation is that it triggers humans' natural empathy for one another
* No human can pretend that they're perfectly selfish, so we instinctively start hunting for way to make a cooperate-cooperate outcome happen
* To get a true sense of a prisoners' dilemma, we need to be in a situation where we're instinctively looking for ways to trick the other side into cooperating while we defect
* There has to be a moral justification for defection in order for us to truly feel the conflict inherent in the prisoners' dilemma