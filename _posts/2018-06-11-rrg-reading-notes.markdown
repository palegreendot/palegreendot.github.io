*--
layout: post
title: "June 11 2018 RRG Notes"
date: 2018-06-09 15:00 -0500
categories: rrg_notes
*--

## [Whither Moral Progress](https://www.greaterwrong.com/posts/szAkYJDtXkcSAiHYE/whither-moral-progress)

* If your only measure of progress is to compare the past to the present, then it's easy to imagine you're making progress when in reality you're on a random walk
* One counterargument to the random-walk argument against morality is to say that the future will be more moral than the present, just as the present was more moral than the past
    * But then aren't we just extrapolating?
    * Can you actually imagine a being that is more moral than yourself -- one that believes that something you consider right and just is actually morally wrong?
* It's easy to come up with examples of moral progress, but it's much more difficult to show directionality and explain how directionality is implemented

## [The Gift We Give To Tomorrow](https://www.greaterwrong.com/posts/pGvyqAQw6yqTjpKf4/the-gift-we-give-to-tomorrow)

* People are the product of evolution
* So why are people so nice?
* People do all sorts of things that would seem to be evolutionary disadvantageous
* How do we explain this without postulating some kind of shadowing figure dictating evolution?
* Our aesthetic and moral senses are a result of evolution, the same as our capacity for cruelty and destruction
* It is not a physical miracle that humans turned out to be as moral as they are, but a moral miracle
* It is entirely possible to recognize the kindness of humanity while recognizing the evolutionary origins of that kindness
* Just like life, at one point, arose from non-living matter, so too did creatures with a sense of morality arise from creatures that did not have a sense of morality

## [Could Anything Be Right?](https://www.greaterwrong.com/posts/vy9nnPdwTjSmt5qdb/could-anything-be-right)

* Eliezer, in 1999, thought that building a generic superintelligence would be sufficient to "solve" morality
* Unfortunately, there is no reason to think that a superintelligence would necessarily *have* to think about morality
* You can't compute morality without having some kind of starting point of what is and is not moral
* You can't completely discard all products of evolution when thinking about morality, because that would involve discarding your own brain
* We should be willing to realize that we know at least a little bit about morality, and that little bit can form a starting point from which we improve
* We have to drop the notion that morality is something that "ghost of perfect emptiness" would agree with -- to such an entity, even the question of what is and is not moral is meaningless

## [Existential Angst Factory](https://www.greaterwrong.com/posts/8rdoea3g6QGhWQtmx/existential-angst-factory)

* Most existential angst isn't existential
* It's easy to think that life is inherently unhappy when you've given up on solving the problems that are making you unhappy
* The opposite of happiness isn't sadness, it's boredom
* Most feelings of existential angst can be blamed on people having at least one problem in their lives that they've given up on solving

## [Can Counterfactuals Be True](https://www.greaterwrong.com/posts/dhGGnB2oxBP3m5cBc/can-counterfactuals-be-true)

* It's an interesting question to wonder whether counterfactuals can be true
* We never actually experience counterfactuals
* Nor can we go back in time and replay events with different starting conditions
* So, given that, can we say *anything* in a counterfactual? Even things that are beyond physical causation?
* If we have a "lawful" computational procedure, the outcome of the counterfactual will be the result of that procedure on different starting conditions
* In order to compute counterfactuals, take some of the variables in your causal model, set them to different values, and then compute the final probability distribution
* Any philosophical work that takes counterfactual distributions as given without telling you how to compute those counterfactuals is largely useless
